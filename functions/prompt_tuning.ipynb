{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e8b15791",
            "metadata": {},
            "source": [
                "# Idea Farm - End-to-End Tuning Playground üöú\n",
                "\n",
                "Fine-tune both the **Content Extraction** and **AI Prompts** completely in the browser.\n",
                "\n",
                "### Workflow\n",
                "1. **Setup**: Install libraries and authenticate.\n",
                "2. **Extraction Logic**: Edit the web scraping code (Trafilatura/YouTube).\n",
                "3. **Prompt Template**: Edit the Gemini prompt.\n",
                "4. **Test**: Provide a URL or Text, run the pipeline, and see the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "715d501e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Setup & Auth\n",
                "# Authenticate with Google Cloud\n",
                "from google.colab import auth\n",
                "auth.authenticate_user()\n",
                "\n",
                "# Install Dependencies\n",
                "!pip install google-cloud-aiplatform trafilatura youtube-transcript-api requests --upgrade --quiet\n",
                "\n",
                "import vertexai\n",
                "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
                "import json\n",
                "import requests\n",
                "import trafilatura\n",
                "from youtube_transcript_api import YouTubeTranscriptApi\n",
                "from urllib.parse import urlparse, parse_qs\n",
                "import logging\n",
                "\n",
                "# Initialize Project\n",
                "PROJECT_ID = \"idea-farm-70752\" # @param {type:\"string\"}\n",
                "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
                "\n",
                "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
                "model = GenerativeModel(\"gemini-2.5-flash\")\n",
                "print(f\"‚úÖ Connected to {PROJECT_ID} in {LOCATION}\")\n",
                "\n",
                "# Configure basic logging\n",
                "logging.basicConfig(level=logging.INFO)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43fbaca1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Content Extraction Logic\n",
                "# This code mirrors 'functions/services/content_extractor.py'.\n",
                "# Edit this cell to improve how we scrape text!\n",
                "\n",
                "def extract_content(url: str) -> str | None:\n",
                "    \"\"\"\n",
                "    Extracts content from a URL.\n",
                "    Detects if it's a YouTube video or a regular web page.\n",
                "    \"\"\"\n",
                "    if not url:\n",
                "        return None\n",
                "\n",
                "    try:\n",
                "        # Check for YouTube\n",
                "        video_id = _get_youtube_video_id(url)\n",
                "        if video_id:\n",
                "            print(f\"üé• Detected YouTube video: {video_id}\")\n",
                "            return _get_youtube_transcript(video_id)\n",
                "        \n",
                "        # Default to web page extraction\n",
                "        print(f\"üåê Extracting web page: {url}\")\n",
                "        return _extract_web_page(url)\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Extraction failed for {url}: {e}\")\n",
                "        return None\n",
                "\n",
                "def _get_youtube_video_id(url: str) -> str | None:\n",
                "    \"\"\"Parses YouTube video ID from URL.\"\"\"\n",
                "    parsed = urlparse(url)\n",
                "    if parsed.hostname in ('youtu.be', 'www.youtu.be'):\n",
                "        return parsed.path[1:]\n",
                "    if parsed.hostname in ('youtube.com', 'www.youtube.com'):\n",
                "        if parsed.path == '/watch':\n",
                "            return parse_qs(parsed.query).get('v', [None])[0]\n",
                "    return None\n",
                "\n",
                "def _get_youtube_transcript(video_id: str) -> str:\n",
                "    \"\"\"Fetches transcript for a YouTube video.\"\"\"\n",
                "    try:\n",
                "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
                "        # Combine text parts\n",
                "        full_text = \" \".join([entry['text'] for entry in transcript_list])\n",
                "        return f\"YouTube Transcript:\\n\\n{full_text}\"\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Could not get transcript for {video_id}: {e}\")\n",
                "        return None\n",
                "\n",
                "def _extract_web_page(url: str) -> str:\n",
                "    \"\"\"\n",
                "    Extracts main text content from a web page using Trafilatura.\n",
                "    Uses requests with a browser-like User-Agent to bypass basic bot filters.\n",
                "    \"\"\"\n",
                "    headers = {\n",
                "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
                "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
                "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
                "        \"Referer\": \"https://www.google.com/\"\n",
                "    }\n",
                "    \n",
                "    downloaded = None\n",
                "    try:\n",
                "        print(f\"Fetching URL: {url}\")\n",
                "        response = requests.get(url, headers=headers, timeout=10)\n",
                "        response.raise_for_status()\n",
                "        downloaded = response.text\n",
                "        print(f\"Downloaded RAW content length: {len(downloaded) if downloaded else 0}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Requests failed. Retrying with Trafilatura fetch... ({e})\")\n",
                "        downloaded = trafilatura.fetch_url(url)\n",
                "    \n",
                "    if not downloaded:\n",
                "        raise ValueError(\"Failed to fetch URL content\")\n",
                "    \n",
                "    text = trafilatura.extract(downloaded, include_comments=False)\n",
                "    if not text:\n",
                "        raise ValueError(\"No text extracted (empty result)\")\n",
                "        \n",
                "    print(f\"‚úÖ Extracted Text Length: {len(text)} chars\")\n",
                "    return text\n",
                "\n",
                "print(\"‚úÖ Extraction Logic Loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c833eec",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Prompt Template\n",
                "# Edit the Gemini prompt below. Keep {content} placeholder.\n",
                "\n",
                "prompt_template = \"\"\"\n",
                "Analyze the following text and provide a structured JSON response.\n",
                "\n",
                "Text:\n",
                "{content}  # Truncate\n",
                "\n",
                "Output Format (JSON):\n",
                "{{\n",
                "    \"overview\": \"A concise paragraph summary (3-5 sentences) suitable for quick reading.\",\n",
                "    \"detailedAnalysis\": \"A comprehensive, 1-2 page deep dive into the content. Use Markdown formatting (## Headers, - bullets, **bold**). Highlight key insights, arguments, and context.\",\n",
                "    \"topic\": \"Suggested Category\",\n",
                "    \"suggestedLinks\": [\n",
                "        {{ \"title\": \"Link Title\", \"url\": \"https://example.com\", \"description\": \"Why relevant\" }}\n",
                "    ]\n",
                "}}\n",
                "\"\"\"\n",
                "print(\"üìù Prompt Template Updated\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "07b670e3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Run Pipeline\n",
                "# Enter a URL OR paste raw text.\n",
                "\n",
                "target_url = \"https://example.com\" # @param {type:\"string\"}\n",
                "raw_text_override = \"\" # @param {type:\"string\"}\n",
                "\n",
                "final_input_text = \"\"\n",
                "\n",
                "if raw_text_override.strip():\n",
                "    print(\"üìÑ Using Raw Text Override\")\n",
                "    final_input_text = raw_text_override\n",
                "elif target_url:\n",
                "    print(f\"üîó Processing URL: {target_url}\")\n",
                "    extracted = extract_content(target_url)\n",
                "    if extracted:\n",
                "        final_input_text = extracted\n",
                "    else:\n",
                "        print(\"‚ùå Extraction failed. Stopping.\")\n",
                "else:\n",
                "    print(\"‚ùå Please provide a URL or Text.\")\n",
                "\n",
                "if final_input_text:\n",
                "    print(\"\\n‚è≥ Generating Summary with Gemini...\")\n",
                "    try:\n",
                "        # inject content\n",
                "        final_prompt = prompt_template.format(content=final_input_text[:15000]) # Increased limit for notebook\n",
                "        \n",
                "        responses = model.generate_content(\n",
                "            final_prompt,\n",
                "            generation_config=GenerationConfig(\n",
                "                temperature=0.2,\n",
                "                max_output_tokens=8192,\n",
                "                top_p=0.8,\n",
                "                top_k=40,\n",
                "                response_mime_type=\"application/json\"\n",
                "            ),\n",
                "            stream=False\n",
                "        )\n",
                "        \n",
                "        # Parse and Display\n",
                "        result_text = responses.text\n",
                "        cleaned_text = result_text.replace('```json', '').replace('```', '').strip()\n",
                "        result_json = json.loads(cleaned_text)\n",
                "        \n",
                "        print(\"\\n‚úÖ Generation Complete!\\n\")\n",
                "        print(json.dumps(result_json, indent=2))\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå AI Generation Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
